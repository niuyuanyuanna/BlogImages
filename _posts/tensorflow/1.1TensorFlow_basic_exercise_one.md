---
title: TensorFlow基础实验一
date: 2018-12-06 20:25:23
tags:
- Tensorflow
- 入门教程
categories: Framework
top_img: https://github.com/niuyuanyuanna/BlogImages/raw/master/background/tf.jpeg
---

# <font color = blue> TensorFlow基础实验一</font>

## 实验目的
通过学习TensorFlow基础入门，掌握TensorFlow的基本使用方法。在此次实验中，你需要掌握这些技巧：

- 初始化变量
- 创建会话
- 训练算法
- 搭建一个神经网络

## 实验内容
### TensorFlow Library
为了使用TensorFlow，首先需要引入TensorFlow库。
```
import math
import numpy as np
import h5py
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.python.framework import ops

np.random.seed(1)
```
引入相应所需的库之后，可以使用TensorFlow实现不同的应用。首先从一个简单的例子开始，计算训练数据的损失。
$loss = \mathcal{L}(\hat{y}, y) = (\hat y^{(i)} - y^{(i)})^2 \tag{1} $

```
y_hat = tf.constant(36, name='y_hat')            # 定义 y_hat为常量，设置其值为36
y = tf.constant(39, name='y')                    # 定义 y为常量，设置其值为39

loss = tf.Variable((y - y_hat)**2, name='loss')  # 定义损失函数为变量

init = tf.global_variables_initializer()         # 初始化，在后面用到 (session.run(init))
                                                 # 损失函数将会被初始化并加入计算图中
with tf.Session() as session:                    # 创建session输出结果
    session.run(init)                            # 初始化变量
    print(session.run(loss))                     # 输出损失
```
此段代码运行结果为：
```
9
```

在TensorFlow中执行程序的一般步骤为：
1. 创建还未被执行的张量
2. 写出张量之间的操作，如表达式
3. 初始化张量
4. 创建会话（Session）
5. 运行会话，这时会运行之前写好的操作

因此，当我们为损失创建一个变量时，我们简单地将损失定义为其他数量的函数，但没有计算它的值。为了计算它，我们必须运行init = tf.global_variables_initializer（）。初始化损失变量，并在最后一行中，我们终于能够计算损失的价并打印其值。

#### session
再来看一个简单的例子。试运行下列代码：
```
a = tf.constant(2)
b = tf.constant(10)
c = tf.multiply(a,b)
print(c)
```
此段代码运行结果为：
```
Tensor("Mul:0", shape=(), dtype=int32)
```
这里并没有输出20，输出结果为一个张量，没有shape属性，且类型为“ini32”。当前你的操作只是将其放置在“计算图”中，并没有运行这个计算。为了能实际的进行运算，需要创建一个会话运行它。
```
sess = tf.Session()
print(sess.run(c))
```
此段代码运行结果为：
```
20
```
总结：记得初始化你的变量，创建一个会话并在会话中运行这些操作。

#### placeholder
接下来，需要了解placeholders。placeholder是一个对象，其值只能在稍后指定。要指定占placeholder的值，可以使用“feed字典”（feed_dict变量）传入值。下面，我们为x创建了一个placeholder。这允许我们稍后在运行会话时传入一个数字。
```
x = tf.placeholder(tf.int64, name = 'x')
print(sess.run(2 * x, feed_dict = {x: 3}))
sess.close()
```
此段代码运行结果为：
```
6
```
当第一次定义x时，不必为它指定一个值。placeholder只是一个变量，您将在稍后运行会话时将数据分配给该变量。在运行会话时向这些placeholder提供数据。当指定计算所需的操作时，告诉TensorFlow如何构建计算图。计算图可以有一些placeholder，其值将在稍后指定。最后，当你运行会话时，告诉TensorFlow执行计算图。

### 线性函数
首先从简单的线性函数开始，计算这个等式：$Y = WX+b$，其中$W$和$X$都是随机取值的矩阵，$b$是一个数值随机的向量。
练习：计算$WX+b$，其中$W$、$X$和$b$取值于随机正态分布。$W$是4×3的矩阵，$X$是3×1的矩阵，$b$为4×1的向量。作为例子，给出定义一个3×1的常量矩阵$X$：
```
X = tf.constant(np.random.randn(3,1), name = "X")
```
你可以使用这些函数：

- tf.matmul(..., ...) 矩阵乘法
- tf.add(..., ...) 矩阵加法
- np.random.randn(...) 随机初始化
```
# GRADED FUNCTION: linear_function

def linear_function():
    """
    定义线性函数: 
            初始化W为一个4×3的随机张量
            初始化X为一个3×1的随机张量
            初始化b为一个4×1的随机张量
    返回值: 
    result -- Y = WX + b 
    """
    
    np.random.seed(1)
    
    ### 填写代码 ###
    X = None
    W = None
    b = None
    Y = None
    ### 完成代码 ### 
    
    # tf.Session()创建会话，使用sess.run(...)计算变量
    
    ### 填写代码 ###
    sess = None
    result = None
    ### 完成代码 ### 
    
    # 关闭session 
    sess.close()

    return result
```
```
print( "result = " + str(linear_function()))
```
此段代码运行结果为：
```
result	[[-2.15657382] [ 2.95891446] [-1.08926781] [-0.84538042]]
```

### sigmoid函数
Tensorflow提供了各种常用的神经网络功能，如tf.sigmoid和tf.softmax。 对于这个练习，我们计算输入的S形函数。
使用placeholder变量x执行此练习。 运行会话时，应该使用feed字典传入输入z。 在本练习中，需要（i）创建占位符x，（ii）使用tf.sigmoid定义计算sigmoid所需的操作，然后（iii）运行会话。
练习：创建sigmoid函数，需要用到下面的函数：

- tf.placeholder(tf.float32, name = "...")
- tf.sigmoid(...)
- sess.run(..., feed_dict = {x: z})
注意：有两种方法创建session
方法1：
```
sess = tf.Session()
# 运行变量初始化及相应运算操作
result = sess.run(..., feed_dict = {...})
sess.close() # 关闭session
```
方法2：
```
with tf.Session() as sess: 
    # 运行变量初始化及相应运算操作
    result = sess.run(..., feed_dict = {...})
    # 此方法可以自动关闭session
```

```
# GRADED FUNCTION: sigmoid

def sigmoid(z):
    """
    计算z的sigmoid值
    
    Arguments:
    z -- 输入，可以为具体的数值或向量
    
    Returns: 
    results -- z的sigmoid值
    """
    
    ### 填写代码 ### ( approx. 4 lines of code)
    # 为创建一个placeholder为其命名为'x'.
    x = None

    # 计算sigmoid(x)
    sigmoid = None

    # 创建session并运行 
    # 使用feed_dict将z的值传递给x 
    None
        # 运行session
        result = None
    
    ### 完成代码 ###
    
    return result
```

```
print ("sigmoid(0) = " + str(sigmoid(0)))
print ("sigmoid(12) = " + str(sigmoid(12)))
```

此段代码运行结果为：
```
sigmoid(0) = 0.5
sigmoid(12) = 0.999994
```

总结：需要掌握
1. 创建placeholder
2. 指定要计算的操作所对应的计算图
3. 创建会话
4. 运行会话，必要时使用feed_dict来指定placeholder变量的值

### 代价函数
在TensorFlow中可以使用内置函数来计算神经网络的代价函数值。不需要编写公式来实现代价函数。
$J = - \frac{1}{m}  \sum_{i = 1}^m  \large ( \small y^{(i)} \log a^{ [2] (i)} + (1-y^{(i)})\log (1-a^{ [2] (i)} )\large )\small\tag{2}$
在TensorFlow中仅需一行代码便可以实现上述功能。

练习：引入交叉熵损失，需要用到下面的函数：

- tf.nn.sigmoid_cross_entropy_with_logits(logits = ...,  labels = ...)

代码应输入z，计算sigmoid，得到a值，然后计算交叉熵损失J。所有的操作都可以通过调用一次tf.nn.sigmoid_cross_entropy_with_logits完成，其计算过程为：
$- \frac{1}{m}  \sum_{i = 1}^m  \large ( \small y^{(i)} \log \sigma(z^{[2](i)}) + (1-y^{(i)})\log (1-\sigma(z^{[2](i)})\large )\small\tag{3}$

```
# GRADED FUNCTION: cost

def cost(logits, labels):
    """
    使用交叉熵作为损失函数，并计算其值
    
    Arguments:
    logits -- 包含z的矢量，最后一个线性单元的输出（在最终的sigmoid激活之前）
    labels -- vector of labels y (1 or 0) 
    
  Notes：在这里“z”和“y”分别为“logits”和“labels”，在在TensorFlow文档中，logits将输入到z中，并标记为y。
    
    
    Returns:
    cost -- 运行session计算得出的cost
    """
    
    ### 填写代码 ### 
    
    # 为z和y创建placeholder
    z = None
    y = None
    
    # 计算损失函数
    cost = None
    
    # 采用第一种方法创建session
    sess = None
    
    # 运行session
    cost = None
    
    # 采用第一种方法关闭session
    None
    
    ### 完成代码 ###
    
    return cost
```

### 使用one hot编码
经过多次深度学习之后，得到的y向量会有C个值，且C是累=类的数量。如果C为4，那么可能得到以下的y向量，需要按照以下方式转换：

<center>
<img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/tensorflow/78291355.jpg" width=75%/>
</center>

这被称为“独热”编码，因为在转换的表示中，每列的一个元素恰恰是“热”（意思是设置为1）。要在numpy中进行这种转换，可能需要编写几行代码。在TensorFlow中，可以使用一行代码：

- tf.one_hot(labels, depth, axis)

练习：执行下面的函数，取一个标签向量和CC类总数，返回一个热独编码。使用tf.one_hot（）来做到这一点。
```
# GRADED FUNCTION: one_hot_matrix

def one_hot_matrix(labels, C):
    """
    创建一个矩阵，其中第i行对应于第i个类，第j列对应于第j个训练样例。所以，如果例子j有一个标签i，则输入（i，j）的值为1。
                     
    Arguments:
    labels -- 包含标签的向量
    C -- 类别的数量，one-hot编码的维度
    
    Returns: 
    one_hot -- one hot矩阵
    """
    
    ### 填写代码 ###
    
    # Create a tf.constant equal to C (depth), name it 'C'. (approx. 1 line)
    C = None
    
    # Use tf.one_hot, be careful with the axis (approx. 1 line)
    one_hot_matrix = None
    
    # Create the session (approx. 1 line)
    sess = None
    
    # Run the session (approx. 1 line)
    one_hot = None
    
    # Close the session (approx. 1 line). See method 1 above.
    None
    
    ### 完成代码 ###
    
    return one_hot
```

```
labels = np.array([1,2,3,0,2,1])
one_hot = one_hot_matrix(labels, C = 4)
print ("one_hot = " + str(one_hot))
```

此段代码运行结果为：
```
one_hot = [[ 0.  0.  0.  1.  0.  0.]
 [ 1.  0.  0.  0.  0.  1.]
 [ 0.  1.  0.  0.  1.  0.]
 [ 0.  0.  1.  0.  0.  0.]]
```

### 使用0和1初始化
现在学习如何初始化一个零和一个向量。需要调用的函数是tf.ones（）。要用零初始化，可以使用tf.zeros（）来代替。这些函数输入一个形状，并分别返回一个同样形状其元素值为0和1的数组。

练习：实现下面的函数来获取形状并返回一个数组（形状的维数）。

- tf.ones(shape)
```
# GRADED FUNCTION: ones

def ones(shape):
    """
    创建一个值全为1的数组
    
    Arguments:
    shape -- 数组的形状
        
    Returns: 
    ones -- 数组元素仅为1
    """
    
    ### 填写代码 ###
    
    # 用tf.ones(...)创建“ones”
    ones = None
    
    # 创建session 
    sess = None
    
    # 运行session计算ones
    ones = None
    
    # 关闭session
    None
    
    ### 完成代码 ###
    return ones
```

```
print ("ones = " + str(ones([3])))
```
此段代码运行结果为：
```
ones = [ 1.  1.  1.]

```
